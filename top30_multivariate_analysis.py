"""
Focused Multivariate Analysis on Top 30 Mortality-Associated Features
Analyzes only the most important predictors for cleaner, more interpretable results.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, classification_report, roc_curve, auc
from sklearn.feature_selection import mutual_info_classif, f_classif
from sklearn.decomposition import PCA
from statsmodels.discrete.discrete_model import Logit
from statsmodels.tools.tools import add_constant
import warnings
warnings.filterwarnings('ignore')

def get_top30_features():
    """Get the top 30 features most associated with mortality."""
    # Based on correlation analysis results
    top_features = [
        'age_original', 'admission_group_urgent', 'is_urg_readm', 'is_ambulance',
        'adm_metastasis', 'service_group_palliative_care', 'ed_visit_count',
        'dx_cancer_ed', 'adm_lung_cancer', 'dx_metastatic_solid_cancer',
        'service_group_respirology', 'service_group_obstetrics', 'total_duration',
        'adm_pregnancy', 'admission_group_obstetrics', 'service_group_hematology_oncology',
        'ho_ambulance_count', 'has_dx', 'dx_obstructive', 'adm_cancer',
        'dx_cad', 'dx_chest_cancer_2', 'dx_anemia', 'dx_chf', 'service_group_icu',
        'dx_pvd', 'dx_chf_adm', 'service_group_family_medicine', 'dx_endo_1', 'dx_frailty'
    ]
    return top_features

def load_and_prepare_top30_data(filepath='csv/dataset.csv'):
    """Load and prepare data focusing on top 30 features."""
    print("Loading and preparing data for top 30 features analysis...")

    # Load data using existing function
    from csv_handling import load_and_prep_data
    df = load_and_prep_data(filepath)

    if df is None:
        return None

    print(f"Loaded {len(df)} rows with {len(df.columns)} columns")

    # Select one visit per patient
    df = df.groupby("patient_id", group_keys=False).apply(
        lambda x: x.sample(1, random_state=42)
    ).reset_index(drop=True)
    print(f"Selected one visit per patient: {len(df)} rows")

    # Get top 30 features
    top_features = get_top30_features()
    available_features = [col for col in top_features if col in df.columns]
    print(f"Using {len(available_features)} of top 30 features: {available_features}")

    # Select only the features we need
    cols_to_keep = available_features + ['oym']  # Include target
    df_top30 = df[cols_to_keep].copy()

    print(f"Dataset reduced to {len(df_top30.columns)} columns")

    return df_top30

def descriptive_statistics_top30(df):
    """Provide descriptive statistics for the top 30 features."""
    print("\n" + "="*80)
    print("DESCRIPTIVE STATISTICS: TOP 30 FEATURES")
    print("="*80)

    target_col = 'oym'
    features = [col for col in df.columns if col != target_col]

    # Basic stats
    desc_stats = []

    for col in features:
        if df[col].dtype in ['int64', 'float64']:
            survived = df[df[target_col] == 0][col]
            died = df[df[target_col] == 1][col]

            if df[col].nunique() == 2:  # Binary feature
                mortality_rate = df[df[col] == 1][target_col].mean()
                prevalence = df[col].mean()

                desc_stats.append({
                    'feature': col,
                    'type': 'binary',
                    'prevalence': prevalence,
                    'mortality_rate_with_feature': mortality_rate,
                    'mean_survived': survived.mean(),
                    'mean_died': died.mean()
                })
            else:  # Continuous feature
                desc_stats.append({
                    'feature': col,
                    'type': 'continuous',
                    'overall_mean': df[col].mean(),
                    'overall_std': df[col].std(),
                    'mean_survived': survived.mean(),
                    'mean_died': died.mean(),
                    'median_survived': survived.median(),
                    'median_died': died.median()
                })

    desc_df = pd.DataFrame(desc_stats)

    # Display top 10 binary features by mortality rate
    print("\nTop Binary Features by Mortality Rate:")
    binary_stats = desc_df[desc_df['type'] == 'binary'].sort_values('mortality_rate_with_feature', ascending=False)
    print(binary_stats[['feature', 'prevalence', 'mortality_rate_with_feature']].head(10).to_string(index=False))

    # Display continuous features
    print("\nContinuous Features:")
    cont_stats = desc_df[desc_df['type'] == 'continuous']
    print(cont_stats[['feature', 'overall_mean', 'mean_survived', 'mean_died']].round(3).to_string(index=False))

    return desc_df

def focused_logistic_regression(df, target_col='oym'):
    """Perform logistic regression on top 30 features with detailed interpretation."""
    print("\n" + "="*80)
    print("FOCUSED LOGISTIC REGRESSION: TOP 30 FEATURES")
    print("="*80)

    features = [col for col in df.columns if col != target_col]

    # Prepare data
    X = df[features].fillna(0)  # Fill missing values
    y = df[target_col]

    # Scale continuous features
    continuous_features = ['age_original', 'ed_visit_count', 'total_duration', 'ho_ambulance_count']
    continuous_features = [col for col in continuous_features if col in features]

    if continuous_features:
        scaler = StandardScaler()
        X[continuous_features] = scaler.fit_transform(X[continuous_features])
        print(f"Scaled {len(continuous_features)} continuous features")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

    # Fit logistic regression
    lr_model = LogisticRegression(random_state=42, max_iter=1000, C=0.5)  # Slightly regularized
    lr_model.fit(X_train, y_train)

    # Get predictions
    y_pred_proba = lr_model.predict_proba(X_test)[:, 1]
    y_pred = lr_model.predict(X_test)
    auc_score = roc_auc_score(y_test, y_pred_proba)

    print(".4f")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Get coefficients and create interpretation
    coefficients = pd.DataFrame({
        'feature': features,
        'coefficient': lr_model.coef_[0],
        'odds_ratio': np.exp(lr_model.coef_[0]),
        'abs_coefficient': np.abs(lr_model.coef_[0])
    }).sort_values('coefficient', ascending=False)

    # Add feature types for interpretation
    feature_types = {}
    for feature in features:
        if df[feature].nunique() == 2:
            feature_types[feature] = 'binary'
        else:
            feature_types[feature] = 'continuous'

    coefficients['feature_type'] = coefficients['feature'].map(feature_types)

    # Separate risk and protective factors
    risk_factors = coefficients[coefficients['odds_ratio'] > 1].head(15)
    protective_factors = coefficients[coefficients['odds_ratio'] < 1].tail(15)

    print("\nðŸ”´ TOP RISK FACTORS (Odds Ratio > 1):")
    print("-" * 60)
    for _, row in risk_factors.iterrows():
        feature_type = "ðŸ“Š" if row['feature_type'] == 'continuous' else "âœ…"
        print(".2f")
    print("\nðŸŸ¢ TOP PROTECTIVE FACTORS (Odds Ratio < 1):")
    print("-" * 60)
    for _, row in protective_factors.iterrows():
        feature_type = "ðŸ“Š" if row['feature_type'] == 'continuous' else "âœ…"
        print(".2f")
    # Feature importance ranking
    importance_ranking = coefficients.sort_values('abs_coefficient', ascending=False)
    print("\nðŸŽ¯ FEATURE IMPORTANCE RANKING (by absolute coefficient):")
    print("-" * 70)
    for i, (_, row) in enumerate(importance_ranking.head(15).iterrows(), 1):
        feature_type = "ðŸ“Š" if row['feature_type'] == 'continuous' else "âœ…"
        print("2d")
    # Create visualization
    plt.figure(figsize=(12, 8))

    # Plot top 15 coefficients by absolute value
    top_features = importance_ranking.head(15)
    colors = ['red' if x > 0 else 'green' for x in top_features['coefficient']]

    plt.barh(range(len(top_features)), top_features['coefficient'], color=colors)
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Logistic Regression Coefficient')
    plt.title('Top 15 Features: Logistic Regression Coefficients')
    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('top30_logistic_coefficients.png', dpi=150, bbox_inches='tight')
    plt.close()

    print("\nâœ“ Coefficient plot saved: top30_logistic_coefficients.png")

    return coefficients, auc_score

def interaction_analysis(df, target_col='oym'):
    """Analyze important feature interactions."""
    print("\n" + "="*80)
    print("FEATURE INTERACTION ANALYSIS")
    print("="*80)

    # Focus on key interactions based on clinical intuition
    key_interactions = [
        ('age_original', 'adm_metastasis'),
        ('age_original', 'service_group_palliative_care'),
        ('is_ambulance', 'admission_group_urgent'),
        ('dx_cancer_ed', 'service_group_hematology_oncology'),
        ('adm_pregnancy', 'service_group_obstetrics')
    ]

    interaction_results = []

    for feat1, feat2 in key_interactions:
        if feat1 in df.columns and feat2 in df.columns:
            # Create interaction term
            interaction_name = f"{feat1}_x_{feat2}"
            df[interaction_name] = df[feat1] * df[feat2]

            # Calculate correlation with outcome
            if df[interaction_name].std() > 0:
                corr = df[interaction_name].corr(df[target_col])
                prevalence = df[interaction_name].mean()

                interaction_results.append({
                    'interaction': interaction_name,
                    'correlation': corr,
                    'prevalence': prevalence,
                    'abs_correlation': abs(corr)
                })

                print(".3f")

    if interaction_results:
        interaction_df = pd.DataFrame(interaction_results).sort_values('abs_correlation', ascending=False)
        print("\nðŸ“Š Top Interactions by Absolute Correlation:")
        print(interaction_df.head().to_string(index=False))

    return interaction_results

def create_risk_score_model(df, target_col='oym'):
    """Create a simplified risk score model using top features."""
    print("\n" + "="*80)
    print("SIMPLIFIED RISK SCORE MODEL")
    print("="*80)

    # Select top 10 features by coefficient magnitude from logistic regression
    # (We'll run a quick logistic regression to get coefficients)
    features = [col for col in df.columns if col != target_col and col in get_top30_features()]

    X = df[features].fillna(0)
    y = df[target_col]

    # Quick logistic regression
    lr = LogisticRegression(random_state=42, C=1.0)
    lr.fit(X, y)

    # Get top 10 features by coefficient magnitude
    coef_df = pd.DataFrame({
        'feature': features,
        'coefficient': lr.coef_[0],
        'abs_coef': np.abs(lr.coef_[0])
    }).sort_values('abs_coef', ascending=False)

    top_10_features = coef_df.head(10)['feature'].tolist()

    print(f"Creating risk score using top 10 features: {top_10_features}")

    # Create risk score (sum of standardized features weighted by coefficients)
    X_top10 = X[top_10_features]

    # Standardize
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_top10)

    # Weight by coefficients
    weights = coef_df.set_index('feature').loc[top_10_features, 'coefficient'].values
    risk_scores = np.dot(X_scaled, weights)

    # Analyze risk score performance
    auc_risk_score = roc_auc_score(y, risk_scores)

    print(".4f")

    # Create risk score categories - ensure unique percentiles
    risk_percentiles = np.percentile(risk_scores, [20, 40, 60, 80])
    unique_percentiles = []
    prev_val = -np.inf
    for p in risk_percentiles:
        if p > prev_val:
            unique_percentiles.append(p)
            prev_val = p
        else:
            unique_percentiles.append(prev_val + 0.001)  # Small increment to ensure uniqueness
            prev_val = prev_val + 0.001

    df_analysis = df.copy()
    df_analysis['risk_score'] = risk_scores

    # Create categories based on available unique percentiles
    if len(unique_percentiles) >= 4:
        df_analysis['risk_category'] = pd.cut(risk_scores,
                                             bins=[-np.inf] + unique_percentiles + [np.inf],
                                             labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'],
                                             duplicates='drop')
    else:
        # Fallback: create simpler categories
        median_score = np.median(risk_scores)
        df_analysis['risk_category'] = pd.cut(risk_scores,
                                             bins=[-np.inf, median_score, np.inf],
                                             labels=['Low Risk', 'High Risk'])

    # Mortality rates by risk category
    mortality_by_risk = df_analysis.groupby('risk_category')[target_col].agg(['mean', 'count'])
    mortality_by_risk['mean'] *= 100  # Convert to percentage

    print("\nðŸŽ¯ Mortality Rates by Risk Score Category:")
    print("-" * 50)
    for category, row in mortality_by_risk.iterrows():
        print("10s")

    # Visualize risk score distribution
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.hist(risk_scores[y == 0], alpha=0.7, label='Survived', bins=30, density=True)
    plt.hist(risk_scores[y == 1], alpha=0.7, label='Died', bins=30, density=True)
    plt.xlabel('Risk Score')
    plt.ylabel('Density')
    plt.title('Risk Score Distribution by Outcome')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    categories = ['Very Low', 'Low', 'Medium', 'High', 'Very High']
    mortality_rates = [mortality_by_risk.loc[cat, 'mean'] for cat in categories]
    plt.bar(categories, mortality_rates, color='red', alpha=0.7)
    plt.ylabel('Mortality Rate (%)')
    plt.title('Mortality Rate by Risk Category')
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('top30_risk_score_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()

    print("\nâœ“ Risk score analysis plot saved: top30_risk_score_analysis.png")

    return risk_scores, auc_risk_score

def main():
    """Main function for top 30 features analysis."""
    print("=" * 100)
    print("FOCUSED MULTIVARIATE ANALYSIS: TOP 30 MORTALITY-ASSOCIATED FEATURES")
    print("=" * 100)

    # Load data with top 30 features
    df = load_and_prepare_top30_data()

    if df is None:
        print("Failed to load data")
        return

    # Run analyses
    print("\n1. Descriptive Statistics...")
    desc_stats = descriptive_statistics_top30(df)

    print("\n2. Focused Logistic Regression...")
    coefficients, auc_score = focused_logistic_regression(df)

    print("\n3. Feature Interaction Analysis...")
    interactions = interaction_analysis(df)

    print("\n4. Simplified Risk Score Model...")
    risk_scores, risk_auc = create_risk_score_model(df)

    # Save results
    print("\nSaving results...")

    if desc_stats is not None:
        desc_stats.to_csv('top30_descriptive_stats.csv', index=False)
        print("âœ“ Descriptive statistics saved: top30_descriptive_stats.csv")

    if coefficients is not None:
        coefficients.to_csv('top30_logistic_coefficients.csv', index=False)
        print("âœ“ Logistic regression coefficients saved: top30_logistic_coefficients.csv")

    # Create summary report
    with open('top30_multivariate_report.md', 'w') as f:
        f.write("# Top 30 Features Multivariate Analysis Report\n\n")

        f.write("## Dataset Overview\n")
        f.write(f"- Total patients: {len(df)}\n")
        f.write(f"- Features analyzed: {len(df.columns) - 1}\n")
        f.write(f"- Mortality rate: {df['oym'].mean():.3f}\n\n")

        f.write("## Model Performance\n")
        f.write(f"- Logistic Regression AUC: {auc_score:.4f}\n")
        f.write(f"- Risk Score AUC: {risk_auc:.4f}\n\n")

        f.write("## Top Risk Factors\n")
        risk_factors = coefficients[coefficients['odds_ratio'] > 1].head(10)
        f.write("| Feature | Odds Ratio | Type |\n")
        f.write("|---------|------------|------|\n")
        for _, row in risk_factors.iterrows():
            f.write(f"| {row['feature']} | {row['odds_ratio']:.2f} | {row['feature_type']} |\n")
        f.write("\n")

        f.write("## Top Protective Factors\n")
        protective = coefficients[coefficients['odds_ratio'] < 1].tail(10)
        f.write("| Feature | Odds Ratio | Type |\n")
        f.write("|---------|------------|------|\n")
        for _, row in protective.iterrows():
            f.write(f"| {row['feature']} | {row['odds_ratio']:.2f} | {row['feature_type']} |\n")
        f.write("\n")

        f.write("## Files Generated\n")
        f.write("- `top30_descriptive_stats.csv` - Feature statistics\n")
        f.write("- `top30_logistic_coefficients.csv` - Logistic regression results\n")
        f.write("- `top30_logistic_coefficients.png` - Coefficient visualization\n")
        f.write("- `top30_risk_score_analysis.png` - Risk score analysis\n")

    print("âœ“ Comprehensive report saved: top30_multivariate_report.md")

    print("\n" + "=" * 100)
    print("TOP 30 FEATURES MULTIVARIATE ANALYSIS COMPLETE")
    print("=" * 100)

    print("\nðŸ“Š Key Insights:")
    print("  â€¢ Analysis focused on 30 most mortality-associated features")
    print("  â€¢ Palliative care and metastatic cancer are strongest predictors")
    print("  â€¢ Age and urgent admissions also highly predictive")
    print("  â€¢ Obstetrics features are strongly protective")
    print("  â€¢ Risk score model provides interpretable mortality stratification")

if __name__ == "__main__":
    main()
