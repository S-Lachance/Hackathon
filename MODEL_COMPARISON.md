# Detailed Model Comparison

## Side-by-Side Overview

| Aspect | Model 1 (Synthetic) | Model 2 (Original) |
|--------|-------------------|-------------------|
| **Training Data** | Synthetic dataset (subset) | Full original dataset |
| **Data Source** | Privacy-preserving generated data | Real patient records |
| **n_estimators** | 1024 | 1024 |
| **max_features** | 20 | 20 |
| **min_samples_leaf** | 10 | 10 |
| **Class Weight (0)** | 0.8271 (82.71%) | 0.7424 (74.24%) |
| **Class Weight (1)** | 0.1729 (17.29%) | 0.2576 (25.76%) |
| **Model File** | `random_forest_synthetic.joblib` | `random_forest_original.joblib` |
| **Hyperparams File** | `rf_homr_best_hps_synthetic.json` | `rf_homr_best_hps_original.json` |

---

## Architectural Similarities

Both models share the **exact same Random Forest architecture**:

### Forest Structure
- **1,024 trees**: Large ensemble for robust predictions
  - More trees = more stable predictions
  - Reduces variance
  - Computational cost is manageable with parallel processing

### Feature Selection
- **20 features per split**: Controlled randomness
  - Out of ~240+ total features
  - Prevents any single feature from dominating
  - Maintains diversity across trees

### Leaf Size
- **Minimum 10 samples per leaf**: Prevents overfitting
  - Ensures each decision has statistical support
  - Reduces sensitivity to outliers
  - Maintains generalization

### Optimization Process
Both used **RandomizedSearchCV** with:
- 100 random hyperparameter combinations
- 5-fold cross-validation
- AUC-ROC as scoring metric
- Same random seed (42) for reproducibility

---

## Critical Difference: Class Weighting Strategy

### Why Class Weights Matter

In mortality prediction:
- **Class Imbalance**: Most patients survive (class 0)
- **Asymmetric Costs**: Missing a death (FN) is worse than a false alarm (FP)
- **Solution**: Adjust class weights to penalize certain errors more

### Model 1 Strategy (Synthetic)

```
Class 0 (No Mortality): 82.71%
Class 1 (Mortality):    17.29%
```

**Interpretation**:
- **Conservative approach** to flagging mortality risk
- Prioritizes specificity (not over-alarming)
- Lower recall (might miss some high-risk cases)
- Reflects synthetic data characteristics

**When this makes sense**:
- Resources for follow-up are limited
- False alarms have significant cost
- Synthetic data has different noise profile
- Training to avoid overfitting to artifacts

### Model 2 Strategy (Original)

```
Class 0 (No Mortality): 74.24%
Class 1 (Mortality):    25.76%
```

**Interpretation**:
- **Aggressive approach** to catching mortality risk
- Prioritizes recall (catching more deaths)
- Lower specificity (more false alarms)
- Reflects real-world clinical priorities

**When this makes sense**:
- Missing a death is very costly
- Resources exist to handle false alarms
- Real data justifies aggressive screening
- Clinical context demands high sensitivity

### The 49% Difference

```
Relative increase in mortality class weight:
(0.2576 - 0.1729) / 0.1729 = 49%
```

This is **substantial** and suggests:
1. Different optimal operating points for synthetic vs real data
2. Real-world clinical priorities favor catching more cases
3. Synthetic data may have different class distribution
4. Hyperparameter optimization found different optima

---

## What This Tells Us About the Data

### Hypothesis 1: Class Distribution Differs
- Synthetic data generation may alter the ratio of positive to negative cases
- Real dataset might have more edge cases that benefit from aggressive weighting
- Synthetic data might be "cleaner" and need less aggressive weighting

### Hypothesis 2: Feature Informativeness Differs
- Synthetic features might be more discriminative (need less weighting)
- Real data features might be noisier (need more weighting to overcome)
- Pattern complexity differs between synthetic and real data

### Hypothesis 3: Optimal Operating Point Differs
- Clinical utility curves differ for synthetic vs real data
- Cost-benefit analysis yields different thresholds
- Cross-validation led to different optimal points

---

## Expected Performance Patterns

### If Models Perform Similarly (AUC within 0.02)

**What it means**:
- âœ“ Synthetic data captures essential patterns
- âœ“ Privacy-preserving approach is validated
- âœ“ Can use synthetic data for development
- âœ“ Class weighting successfully compensates for differences

**But note**:
- Recall/Specificity trade-offs will differ
- Optimal threshold will differ
- Calibration might differ (predicted probabilities)

### If Original Model Significantly Better (AUC > 0.05 difference)

**What it means**:
- âš  Synthetic data missing important patterns
- âš  Need to improve synthetic data generation
- âš  Some clinical patterns not preserved
- âš  Feature distributions may differ significantly

**Action items**:
- Analyze which features differ most
- Identify missing correlations
- Improve synthetic data generation
- Consider augmenting with real data samples

### If Synthetic Model Better (Surprising!)

**What it means**:
- ðŸ¤” Synthetic data might remove noise
- ðŸ¤” Original model might be overfitting
- ðŸ¤” Class imbalance handled better in synthetic
- ðŸ¤” Real data might have distribution shift

**Action items**:
- Investigate data quality in original set
- Check for temporal drift
- Analyze outliers and anomalies
- Consider ensemble approaches

---

## Evaluation Metrics to Focus On

### 1. AUC-ROC (Primary)
**Why**: Threshold-independent, handles imbalance
- Measures discrimination ability across all possible thresholds
- 0.5 = random guessing, 1.0 = perfect
- Standard metric for binary classification with imbalance

**What to expect**:
- Healthcare models typically achieve 0.70-0.85
- > 0.80 is considered good for mortality prediction
- Difference of 0.02-0.03 is meaningful

### 2. Recall (Sensitivity)
**Why**: Clinical priority is catching high-risk patients
- Of all actual mortality cases, what % did we catch?
- **Most important** for patient safety
- Higher is better, but at cost of specificity

**What to expect**:
- Model 2 should have higher recall (aggressive weighting)
- Trade-off: will have more false positives
- Look for recall > 0.70

### 3. Specificity
**Why**: Resource utilization matters
- Of all survivors, what % did we correctly identify?
- Affects resource allocation and false alarm rate
- Lower means more unnecessary interventions

**What to expect**:
- Model 1 should have higher specificity (conservative)
- Trade-off: will have more false negatives
- Look for specificity > 0.80

### 4. Precision (PPV - Positive Predictive Value)
**Why**: Trust in predictions
- Of all mortality predictions, what % are correct?
- Affects clinician trust in the system
- Low precision = alert fatigue

**What to expect**:
- Challenging with severe imbalance
- Even good models may have precision < 0.50
- Context: if 5% mortality rate, random guessing = 0.05

### 5. F1-Score
**Why**: Balance between precision and recall
- Harmonic mean: 2 * (precision * recall) / (precision + recall)
- Useful when you need one metric for optimization
- Balanced view of performance

**What to expect**:
- Should reflect the precision-recall trade-off
- Model 2 might have higher F1 (better recall)
- Look for F1 > 0.40 as reasonable

---

## Calibration Considerations

### What is Calibration?
Beyond discrimination (AUC), we want predicted probabilities to match actual risk:
- If model predicts 20% mortality risk, we want ~20% of those patients to die
- Miscalibrated models may discriminate well but give misleading probabilities

### Expected Calibration Patterns

**Model 1 (Synthetic)**:
- May be over-confident (probabilities too extreme)
- Synthetic data might be "too clean"
- Conservative weighting might shift probabilities down

**Model 2 (Original)**:
- Should be better calibrated to real-world risks
- Aggressive weighting might shift probabilities up
- Real data noise provides natural calibration

**How to check** (advanced):
- Calibration plots (predicted vs actual)
- Brier score
- Expected Calibration Error (ECE)

---

## Clinical Interpretation Framework

### False Negative (FN): Predicted Low Risk, But Patient Dies
**Clinical Impact**: âš âš âš âš âš  CRITICAL
- Patient sent home or given low-priority care
- Potentially preventable death
- Medical/legal liability
- **Goal**: Minimize these

### False Positive (FP): Predicted High Risk, But Patient Survives
**Clinical Impact**: âš âš  MODERATE
- Unnecessary interventions or monitoring
- Resource waste
- Patient anxiety
- **Goal**: Acceptable level for safety

### True Positive (TP): Correctly Identified High Risk
**Clinical Impact**: âœ“âœ“âœ“âœ“âœ“ EXCELLENT
- Appropriate intervention
- Lives saved
- Optimal resource use

### True Negative (TN): Correctly Identified Low Risk
**Clinical Impact**: âœ“âœ“âœ“ GOOD
- Appropriate discharge/care
- Efficient resource use
- Patient confidence

---

## Decision Threshold Analysis

Both models output probabilities [0, 1]. The default threshold is 0.5, but **this is almost certainly not optimal**.

### Model 1 (Conservative Weighting)
- Default threshold might be too high
- Consider lowering threshold (e.g., 0.3) to catch more cases
- Find threshold that maximizes clinical utility

### Model 2 (Aggressive Weighting)
- Default threshold might be closer to optimal
- Already biased toward sensitivity
- Fine-tune based on cost-benefit analysis

### How to Find Optimal Threshold
1. Plot ROC curve
2. Calculate clinical cost function
3. Find threshold that minimizes expected cost
4. Validate on held-out set

**Example Cost Function**:
```
Cost = C_fn * FN + C_fp * FP
where C_fn >> C_fp (missing death much worse than false alarm)
```

---

## Feature Importance (Next Steps)

After understanding baseline performance, analyze:

### What to Compare
- Top 20 features in each model
- Feature importance rankings
- Correlation patterns
- Distribution differences

### Questions to Answer
- Do models rely on same features?
- Are synthetic feature distributions realistic?
- Which features are missing or distorted?
- Are there unexpectedly important features?

### Tools
```python
# Feature importance
importances = model.feature_importances_
feature_names = feature_cols
sorted_idx = np.argsort(importances)[::-1]

# Plot top 20
plt.barh(range(20), importances[sorted_idx][:20])
plt.yticks(range(20), [feature_names[i] for i in sorted_idx[:20]])
```

---

## Improvement Roadmap

### Short-term (Beat the Baseline)
1. **Better class weighting**: Optimize for clinical cost function
2. **Threshold tuning**: Find optimal decision boundary
3. **Feature engineering**: Create interaction terms
4. **Ensemble**: Combine both models

### Medium-term (Significant Improvements)
1. **XGBoost/LightGBM**: Try gradient boosting
2. **Deep learning**: If dataset is large enough
3. **Better imbalance handling**: SMOTE, focal loss
4. **Cross-validation**: More robust evaluation

### Long-term (Production-Ready)
1. **Calibration**: Ensure probabilities are meaningful
2. **Interpretability**: SHAP values, LIME
3. **Fairness**: Check for demographic biases
4. **Monitoring**: Detect distribution drift

---

## Summary: The Key Question

**"Is synthetic data good enough for this clinical task?"**

The answer depends on the performance gap:
- **Gap < 0.02 AUC**: Yes, synthetic data is excellent
- **Gap 0.02-0.05 AUC**: Maybe, depends on use case
- **Gap > 0.05 AUC**: No, need better synthetic data or real data

Your evaluation will provide this answer and guide next steps.

---

## Quick Reference Commands

```bash
# Setup
pip install -r requirements.txt

# Download models (manual)
# Visit: https://drive.google.com/drive/folders/13r3tEXg5SfyesCLX0w76K5fJ7MHmbJes

# Evaluate
python evaluate_baselines.py

# Generate visualizations (integrate into evaluation)
# See visualize_comparison.py for functions
```

---

## Expected Runtime

- **Data loading**: ~5-10 seconds
- **Model loading**: ~1-2 seconds  
- **Evaluation per model**: ~10-30 seconds
- **Total**: ~1 minute

With 248K samples and 1024 trees per model, this is reasonable.

---

This comparison sets the stage for your evaluation. Run `evaluate_baselines.py` to see how these theoretical differences play out in practice!

